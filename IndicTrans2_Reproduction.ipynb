{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndicTrans2 Reproduction Study\n",
    "\n",
    "ðŸ”¬ **Investigating the Reproducibility of IndicTrans2 Multilingual Translation Models**\n",
    "\n",
    "This notebook reproduces the evaluation of IndicTrans2-Dist-M2M model on the IN22-Gen benchmark and compares results with the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install essential libraries for deep learning, model loading, and evaluation\n",
    "!pip install torch --quiet\n",
    "!pip install transformers==4.53.2 --quiet\n",
    "!pip install datasets==2.18.0 --quiet\n",
    "!pip install evaluate==0.4.1 --quiet\n",
    "!pip install sacrebleu==2.4.2 --quiet\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the custom toolkit required for IndicTrans2 pre-processing and post-processing\n",
    "!pip install indictranstoolkit --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Access the stored Hugging Face token from Kaggle Secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "# Programmatically log in to Hugging Face\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Define the device for computation (GPU if available, otherwise CPU)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "LANGS = [\"hin_Deva\", \"pan_Guru\", \"tam_Taml\", \"ben_Beng\", \"mar_Deva\", \"tel_Telu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"ai4bharat/indictrans2-indic-indic-dist-320M\"\n",
    "\n",
    "# Load the tokenizer\n",
    "# trust_remote_code is required for custom tokenization logic\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Load the model\n",
    "# trust_remote_code is required for custom model architecture\n",
    "# torch_dtype=torch.float16 uses half-precision for memory efficiency and speed\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "model.eval() # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IndicTransToolkit.processor import IndicProcessor\n",
    "\n",
    "# Instantiate the processor for inference tasks\n",
    "ip = IndicProcessor(inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the 'test' split of the IN22-Gen dataset in the 'all' configuration\n",
    "# This provides a single table with all language pairs aligned by sentence ID\n",
    "in22_gen_dataset = load_dataset(\"ai4bharat/IN22-Gen\", split=\"test\")\n",
    "\n",
    "# Display the first example to inspect the structure\n",
    "print(\"Dataset loaded. Example entry:\")\n",
    "print(in22_gen_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def translate_batch(sentences, src_lang, tgt_lang, batch_size=16):\n",
    "    translations = []\n",
    "    for i in tqdm(range(0, len(sentences), batch_size), desc=f\"Translating {src_lang} -> {tgt_lang}\"):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "\n",
    "        if not batch or all(s is None or str(s).strip() == \"\" for s in batch):\n",
    "            continue\n",
    "\n",
    "        # Preprocess\n",
    "        preprocessed_batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "        if preprocessed_batch is None or len(preprocessed_batch) == 0:\n",
    "            print(f\"âš ï¸ Preprocessing returned None/empty for {src_lang}->{tgt_lang}\")\n",
    "            continue\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            preprocessed_batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        forced_bos_token_id = tokenizer.convert_tokens_to_ids(f\"<2{tgt_lang}>\")\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                **inputs,\n",
    "                num_beams=5,\n",
    "                max_length=256,\n",
    "                forced_bos_token_id=forced_bos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        decoded = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Postprocess safely\n",
    "        try:\n",
    "            postprocessed = ip.postprocess_batch(decoded, lang=tgt_lang)\n",
    "            if postprocessed is None:\n",
    "                raise ValueError(\"postprocess_batch returned None\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Postprocessing failed for {src_lang}->{tgt_lang}: {e}\")\n",
    "            postprocessed = decoded  # fallback\n",
    "\n",
    "        translations.extend(postprocessed)\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "def evaluate_direction(src_lang, tgt_lang, dataset, num_samples=200):\n",
    "    \"\"\"\n",
    "    Evaluate translation performance from src_lang -> tgt_lang\n",
    "    using chrF++ on a subset of the dataset.\n",
    "    \"\"\"\n",
    "    src_sentences = dataset[src_lang][:num_samples]\n",
    "    tgt_sentences = dataset[tgt_lang][:num_samples]\n",
    "\n",
    "    # Translate\n",
    "    preds = translate_batch(src_sentences, src_lang, tgt_lang)\n",
    "\n",
    "    # Ensure preds is a flat list of strings\n",
    "    preds = [p if isinstance(p, str) else \"\" for p in preds]\n",
    "\n",
    "    if len(preds) == 0:\n",
    "        print(f\"âš ï¸ No predictions produced for {src_lang}->{tgt_lang}\")\n",
    "        return None\n",
    "\n",
    "    from sacrebleu.metrics import CHRF\n",
    "    chrf = CHRF(word_order=2)  # chrF++\n",
    "    score = chrf.corpus_score(preds, [tgt_sentences]).score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 5 sentences for testing\n",
    "sample_src = in22_gen_dataset[\"hin_Deva\"][:5]\n",
    "sample_tgt = in22_gen_dataset[\"pan_Guru\"][:5]\n",
    "\n",
    "print(\"RAW source examples:\", sample_src)\n",
    "\n",
    "# Step 1: Preprocess\n",
    "preprocessed = ip.preprocess_batch(sample_src, src_lang=\"hin_Deva\", tgt_lang=\"pan_Guru\")\n",
    "print(\"\\nAfter preprocess:\", preprocessed)\n",
    "\n",
    "# Step 2: Tokenize\n",
    "inputs = tokenizer(\n",
    "    preprocessed,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"\\nTokenizer output keys:\", inputs.keys())\n",
    "print(\"Shape of input_ids:\", inputs[\"input_ids\"].shape if \"input_ids\" in inputs else \"MISSING\")\n",
    "\n",
    "# Step 3: Generate (only if inputs look good)\n",
    "if \"input_ids\" in inputs:\n",
    "    # Define the target language and get its token ID\n",
    "    tgt_lang = \"pan_Guru\"\n",
    "    forced_bos_token_id = tokenizer.convert_tokens_to_ids(f\"<2{tgt_lang}>\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Pass the token ID to the generate function\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            num_beams=5, \n",
    "            max_length=256,\n",
    "            forced_bos_token_id=forced_bos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    post = ip.postprocess_batch(decoded, lang=\"pan_Guru\")\n",
    "\n",
    "    print(\"\\nDecoded:\", decoded)\n",
    "    print(\"Postprocessed:\", post)\n",
    "    print(\"\\nTarget (for comparison):\", sample_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the results\n",
    "results_data = []\n",
    "\n",
    "# Loop through all ordered pairs of your LANGS (excluding same-source/target)\n",
    "for src in LANGS:\n",
    "    for tgt in LANGS:\n",
    "        if src == tgt:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Evaluating direction: {src} -> {tgt} ---\")\n",
    "        try:\n",
    "            score = evaluate_direction(src, tgt, in22_gen_dataset)\n",
    "            results_data.append({\n",
    "                \"Source\": src,\n",
    "                \"Target\": tgt,\n",
    "                \"Reproduced chrF++\": round(score, 1)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during {src} -> {tgt}: {e}\")\n",
    "            results_data.append({\n",
    "                \"Source\": src,\n",
    "                \"Target\": tgt,\n",
    "                \"Reproduced chrF++\": \"Error\"\n",
    "            })\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "reproduced_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"\\n--- Full Experiment Complete ---\")\n",
    "reproduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Paper Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate average scores for each language (xx->lang and lang->xx)\n",
    "def calculate_average_scores(results_df):\n",
    "    avg_scores = {}\n",
    "    \n",
    "    for lang in LANGS:\n",
    "        # xx->lang: all sources to this target language\n",
    "        xx_to_lang_scores = results_df[results_df['Target'] == lang]['Reproduced chrF++'].tolist()\n",
    "        \n",
    "        # lang->xx: this source to all target languages  \n",
    "        lang_to_xx_scores = results_df[results_df['Source'] == lang]['Reproduced chrF++'].tolist()\n",
    "        \n",
    "        avg_scores[lang] = {\n",
    "            'xx-lang': np.mean(xx_to_lang_scores) if xx_to_lang_scores else 0,\n",
    "            'lang-xx': np.mean(lang_to_xx_scores) if lang_to_xx_scores else 0\n",
    "        }\n",
    "    \n",
    "    return avg_scores\n",
    "\n",
    "# Calculate our reproduced average scores\n",
    "reproduced_avg_scores = calculate_average_scores(reproduced_df)\n",
    "\n",
    "# Create comparison table with paper scores from the image\n",
    "comparison_data = []\n",
    "for lang in LANGS:\n",
    "    # Paper scores from the table image (IT2-Dist-M2M columns)\n",
    "    paper_xx_lang = {\n",
    "        'hin_Deva': 47.1, 'pan_Guru': 40.9, 'tam_Taml': 42.6, \n",
    "        'ben_Beng': 43.2, 'mar_Deva': 41.5, 'tel_Telu': 42.9\n",
    "    }[lang]\n",
    "    \n",
    "    paper_lang_xx = {\n",
    "        'hin_Deva': 42.3, 'pan_Guru': 39.1, 'tam_Taml': 39.3, \n",
    "        'ben_Beng': 41.2, 'mar_Deva': 42.4, 'tel_Telu': 41.9\n",
    "    }[lang]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Language': lang,\n",
    "        'Paper xx-lang': paper_xx_lang,\n",
    "        'Reproduced xx-lang': round(reproduced_avg_scores[lang]['xx-lang'], 1),\n",
    "        'Paper lang-xx': paper_lang_xx,\n",
    "        'Reproduced lang-xx': round(reproduced_avg_scores[lang]['lang-xx'], 1)\n",
    "    })\n",
    "\n",
    "# Create final comparison DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Calculate differences\n",
    "comparison_df['Diff xx-lang'] = comparison_df['Reproduced xx-lang'] - comparison_df['Paper xx-lang']\n",
    "comparison_df['Diff lang-xx'] = comparison_df['Reproduced lang-xx'] - comparison_df['Paper lang-xx']\n",
    "\n",
    "print(\"Comparison of Average Scores (IT2-Dist-M2M)\")\n",
    "print(\"=\" * 80)\n",
    "display(comparison_df)\n",
    "\n",
    "# Calculate overall statistics\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"Average xx-lang difference: {comparison_df['Diff xx-lang'].mean():.1f}\")\n",
    "print(f\"Average lang-xx difference: {comparison_df['Diff lang-xx'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Performance Gap Analysis\n",
    "- **Average xx-lang difference**: -13.8 chrF++ points\n",
    "- **Average lang-xx difference**: -11.8 chrF++ points\n",
    "- **Largest gaps**: Punjabi translations show the biggest performance drop\n",
    "\n",
    ### Potential Reasons for the Gap\n",
    "1. **Model Configuration**: Potential differences in model loading/inference\n",
    "2. **Library Versions**: transformers==4.53.2 (compatibility-driven choice)\n",
    "3. **Evaluation Methodology**: 200 samples vs. full test set\n",
    "4. **Preprocessing**: Slight variations in text normalization\n",
    "5. **Hardware**: Tesla T4 with FP16 vs. paper's unspecified setup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
